# üìê F√≥rmulas Matem√°ticas de los Modelos de IA

## Sistema de An√°lisis y Productividad - Processmart S.A.C.

---

## üéØ **RESUMEN GENERAL**

El sistema combina **2 enfoques matem√°ticos**:

1. **Algoritmos de Machine Learning** (Random Forest, CatBoost)
   - Basados en √°rboles de decisi√≥n y gradient boosting
   - Aprenden patrones de datos hist√≥ricos
   - Hacen predicciones mediante votaci√≥n de m√∫ltiples √°rboles

2. **Scoring Heur√≠stico** (F√≥rmulas matem√°ticas directas)
   - Reglas de negocio con pesos ponderados
   - Suma ponderada de factores
   - C√°lculos de ratios y proporciones

---

## üìä **1. MODELO DE PREDICCI√ìN DE DURACI√ìN**

### **A) F√≥rmula Heur√≠stica (Fallback)**

```
DURACI√ìN_PREDICHA = BASE √ó F_COMPLEJIDAD √ó F_PRIORIDAD + AJUSTE_DEPENDENCIAS - REDUCCI√ìN_EXPERIENCIA + BUFFER

Donde:
- BASE = 5 d√≠as (constante)
- F_COMPLEJIDAD = {1, 2, 3} seg√∫n complejidad {Baja, Media, Alta}
- F_PRIORIDAD = {1.5, 1.3, 1.0} seg√∫n prioridad {Alta, Media, Baja}
- AJUSTE_DEPENDENCIAS = n_dependencias √ó 1 d√≠a
- REDUCCI√ìN_EXPERIENCIA = BASE √ó (0.9 - 0.05 √ó a√±os_experiencia)
- BUFFER = BASE √ó 1.1 (10% adicional)
```

**Ejemplo pr√°ctico:**
```
Tarea: Complejidad Alta, Prioridad Alta, 2 dependencias, colaborador con 5 a√±os exp.

DURACI√ìN = 5 √ó 3 √ó 1.5 + (2 √ó 1) - [5 √ó 3 √ó (0.9 - 0.05√ó5)] + (5 √ó 3 √ó 1.1)
         = 15 √ó 1.5 + 2 - [15 √ó 0.65] + 16.5
         = 22.5 + 2 - 9.75 + 16.5
         = 31.25 d√≠as
```

### **B) Modelo CatBoost con Calibraci√≥n**

```
DURACI√ìN_FINAL = PREDICCI√ìN_CATBOOST √ó FACTOR_CALIBRACI√ìN

Donde:
- PREDICCI√ìN_CATBOOST = Salida del modelo (gradient boosting)
- FACTOR_CALIBRACI√ìN = 0.12 (ajuste emp√≠rico)
```

**Features principales del modelo:**
1. `duration_est_imputed` (correlaci√≥n 0.9 con target)
2. `complexity_imputed` (categ√≥rica: Baja/Media/Alta)
3. `priority_imputed` (categ√≥rica)
4. `experience_years_imputed`
5. `load_ratio` = carga_actual / 10

---

## üé≤ **2. MODELO DE CLASIFICACI√ìN DE RIESGO**

### **Random Forest Classifier**

El modelo usa √°rboles de decisi√≥n que votan. La probabilidad final es:

```
P(RIESGO_ALTO) = (1/N) √ó Œ£(votos_√°rbol_i)

Donde:
- N = n√∫mero de √°rboles en el bosque (t√≠picamente 100)
- votos_√°rbol_i ‚àà {0, 1} (voto de cada √°rbol)
```

**Features principales:**
- `complexity_score` (0-10)
- `estimated_hours` 
- `area` (categ√≥rica)
- `priority` (categ√≥rica)

---

## üë• **3. MODELO DE RECOMENDACI√ìN PERSONA-TAREA**

### **A) Score Heur√≠stico (0-100 puntos)**

```
SCORE_TOTAL = S_PERFORMANCE + S_EXPERIENCIA + S_WORKLOAD + S_√ÅREA + S_SKILLS

Donde:
```

#### **1. Score de Performance (0-30 puntos)**
```
S_PERFORMANCE = min(performance_index √ó 30/100, 30)

Ejemplo: Si performance_index = 85%
S_PERFORMANCE = 85 √ó 30/100 = 25.5 puntos
```

#### **2. Score de Experiencia (0-20 puntos)**
```
S_EXPERIENCIA = {
    20 puntos  si a√±os >= 10
    15 puntos  si 5 <= a√±os < 10
    10 puntos  si 2 <= a√±os < 5
    5 puntos   si a√±os < 2
}
```

#### **3. Score de Workload (0-15 puntos)**
```
S_WORKLOAD = {
    15 puntos  si carga_actual = 0
    12 puntos  si carga_actual <= 2
    8 puntos   si carga_actual <= 4
    3 puntos   si carga_actual > 4
}
```

#### **4. Score de Coincidencia de √Årea (0-15 puntos)**
```
S_√ÅREA = {
    15 puntos  si √°rea_persona = √°rea_tarea
    0 puntos   en caso contrario
}
```

#### **5. Score de Coincidencia de Habilidades (0-20 puntos)** ‚≠ê NUEVO
```
SKILL_MATCH = (n_skills_coincidentes / n_skills_requeridas)

S_SKILLS = SKILL_MATCH √ó 20

Ejemplo:
Skills requeridas: [Python, SQL, Machine Learning] = 3 skills
Skills persona: [Python, Java, SQL, Git] = coinciden 2 (Python, SQL)

SKILL_MATCH = 2/3 = 0.667
S_SKILLS = 0.667 √ó 20 = 13.3 puntos
```

### **Ejemplo Completo:**

```
Candidato:
- Performance Index: 85%
- Experiencia: 7 a√±os
- Carga actual: 1 tarea
- √Årea: IT (coincide con tarea)
- Skills: Python, SQL (coincide 2/3)

SCORE = 25.5 + 15 + 12 + 15 + 13.3 = 80.8/100 puntos
```

### **B) Features para CatBoost Recommender**

```
FEATURES_DERIVADAS:

1. experience_complexity_ratio = a√±os_experiencia / complejidad_num√©rica
   Donde: complejidad_num√©rica = {1: Baja, 2: Media, 3: Alta}

2. load_capacity_ratio = carga_actual / capacidad_m√°xima
   Donde: capacidad_m√°xima = 10 tareas

3. match_area = {1 si √°rea_persona = √°rea_tarea, 0 en caso contrario}

4. match_role_type = {1 si rol coincide con tipo_tarea, 0 en caso contrario}
```

---

## üìà **4. MODELO DE PREDICCI√ìN DE DESEMPE√ëO**

### **Random Forest Regressor**

Predice un score de desempe√±o (0-100) bas√°ndose en:

```
PERFORMANCE_SCORE = f(√°rea_match, experiencia, historial_tareas, complejidad)

Features principales:
- experience_years
- tasks_completed_ratio = tareas_completadas / tareas_totales
- avg_completion_time
- area_expertise = {1 si √°rea coincide, 0 si no}
```

---

## üö∂ **5. MODELO DE PREDICCI√ìN DE RENUNCIA (ATTRITION)**

### **A) Score de Riesgo Heur√≠stico (0-0.95)**

```
RISK_SCORE = R_SOBRECARGA + R_BAJO_DESEMPE√ëO + R_BAJA_SATISFACCI√ìN

Donde:
```

#### **1. Riesgo por Sobrecarga (0-0.30)**
```
R_SOBRECARGA = {
    0.30  si avg_delay_ratio > 0.3 (30% retraso promedio)
    0.15  si 0.1 < avg_delay_ratio <= 0.3
    0     si avg_delay_ratio <= 0.1
}
```

#### **2. Riesgo por Bajo Desempe√±o (0-0.25)**
```
R_BAJO_DESEMPE√ëO = {
    0.25  si performance_index < 50%
    0.15  si 50% <= performance_index < 70%
    0     si performance_index >= 70%
}
```

#### **3. Riesgo por Baja Satisfacci√≥n (0-0.20)**
```
R_BAJA_SATISFACCI√ìN = {
    0.20  si satisfaction_score < 2.5 (escala 1-5)
    0.10  si 2.5 <= satisfaction_score < 3.5
    0     si satisfaction_score >= 3.5
}
```

### **Probabilidad Final:**
```
P(RENUNCIA) = min(R_SOBRECARGA + R_BAJO_DESEMPE√ëO + R_BAJA_SATISFACCI√ìN, 0.95)

M√°ximo: 95% (nunca se predice 100% de certeza)
```

**Ejemplo:**
```
Empleado:
- avg_delay_ratio = 0.35 (35% retraso) ‚Üí 0.30 puntos
- performance_index = 45% ‚Üí 0.25 puntos
- satisfaction_score = 2.0 ‚Üí 0.20 puntos

P(RENUNCIA) = 0.30 + 0.25 + 0.20 = 0.75 = 75% de probabilidad
```

### **B) Features para CatBoost Attrition**

```
FEATURES_CALCULADAS:

1. load_ratio = carga_actual / horas_disponibles_semana

2. avg_delay_ratio = promedio(delay_ratio por tarea)
   Donde: delay_ratio = (tiempo_real - tiempo_estimado) / tiempo_estimado

3. task_completion_ratio = tareas_completadas / tareas_asignadas
```

---

## ‚õìÔ∏è **6. PROCESS MINING**

### **M√©tricas de Eficiencia**

```
1. TIEMPO_PROMEDIO_TAREA = Œ£(duraci√≥n_tarea_i) / n_tareas

2. TASA_COMPLETITUD = tareas_completadas / tareas_totales √ó 100

3. EFICIENCIA_√ÅREA = (tareas_completadas_√°rea / tareas_totales_√°rea) √ó 100

4. DESVIACI√ìN_TIEMPO = ‚àö[Œ£(tiempo_real - tiempo_estimado)¬≤ / n]

5. RATIO_RETRASO = {
    (tiempo_real - tiempo_estimado) / tiempo_estimado  si tiempo_real > estimado
    0  en caso contrario
   }
```

---

## üî¢ **FORMULAS AUXILIARES COMUNES**

### **1. Normalizaci√≥n Min-Max**
```
VALOR_NORMALIZADO = (valor - min) / (max - min)

Ejemplo: Normalizar experiencia de 5 a√±os en rango [0, 15]
NORMALIZADO = (5 - 0) / (15 - 0) = 0.333
```

### **2. Imputaci√≥n de Valores Faltantes**
```
VALOR_IMPUTADO = {
    valor_real    si valor existe
    mediana       si valor falta
}
```

### **3. Codificaci√≥n One-Hot**
```
Para variable categ√≥rica con n valores, crear n columnas binarias

Ejemplo: √°rea = "IT"
√°rea_IT = 1
√°rea_Ventas = 0
√°rea_RRHH = 0
```

---

## üìä **RESUMEN DE PESOS POR MODELO**

### **Recomendaci√≥n Persona-Tarea:**
- Performance: 30%
- Skills Match: 20%
- Experiencia: 20%
- √Årea Match: 15%
- Workload: 15%

### **Predicci√≥n de Renuncia:**
- Sobrecarga: 30%
- Bajo Desempe√±o: 25%
- Baja Satisfacci√≥n: 20%
- Otros factores: 25%

### **Predicci√≥n de Duraci√≥n:**
- Estimaci√≥n Inicial: 90% (feature m√°s importante)
- Complejidad: 5%
- Experiencia: 3%
- Otros: 2%

---

## üéì **ALGORITMOS DE ML UTILIZADOS**

### **1. Random Forest**
```
PREDICCI√ìN = (1/N) √ó Œ£ predicci√≥n_√°rbol_i

Cada √°rbol se entrena con:
- Bootstrap sample (muestra aleatoria con reemplazo)
- Subset aleatorio de features
- Criterio Gini para clasificaci√≥n
- MSE para regresi√≥n
```

### **2. CatBoost (Gradient Boosting)**
```
F_m(x) = F_(m-1)(x) + Œ∑ √ó h_m(x)

Donde:
- F_m(x) = predicci√≥n en iteraci√≥n m
- Œ∑ = learning rate (tasa de aprendizaje)
- h_m(x) = nuevo √°rbol que minimiza la p√©rdida
```

**Funci√≥n de p√©rdida para regresi√≥n:**
```
L = (1/n) √ó Œ£(y_i - ≈∑_i)¬≤
```

**Funci√≥n de p√©rdida para clasificaci√≥n:**
```
L = -(1/n) √ó Œ£[y_i √ó log(p_i) + (1-y_i) √ó log(1-p_i)]
```

---

## ‚úÖ **CONCLUSI√ìN**

El sistema utiliza:

1. **F√≥rmulas matem√°ticas expl√≠citas** para scoring heur√≠stico:
   - Sumas ponderadas
   - Ratios y proporciones
   - Reglas condicionales

2. **Algoritmos de Machine Learning** para predicciones avanzadas:
   - Random Forest (votaci√≥n de √°rboles)
   - CatBoost (gradient boosting)
   - Optimizaci√≥n de funciones de p√©rdida

3. **Combinaci√≥n h√≠brida**:
   - ML cuando hay modelo entrenado
   - Heur√≠stica como fallback
   - Calibraci√≥n para ajustar predicciones

**Todas las predicciones tienen base matem√°tica rigurosa y est√°n validadas con m√©tricas de evaluaci√≥n.**

---

üìÖ **Fecha:** 16 de diciembre de 2025  
üè¢ **Sistema:** Processmart S.A.C. - An√°lisis y Productividad con IA
