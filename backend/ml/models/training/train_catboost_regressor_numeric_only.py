"""
Entrenamiento de Modelo CatBoost para Regresi√≥n de Duraci√≥n - SOLO FEATURES NUM√âRICAS
=======================================================================================
Predicci√≥n de duration_real (tiempo real que toma completar una tarea).

üéØ VERSI√ìN SIMPLIFICADA - Compatible con cualquier dominio
============================================================

Caracter√≠sticas principales:
- SOLO features num√©ricas (sin dependencias categ√≥ricas de dominio espec√≠fico)
- Transformaci√≥n logar√≠tmica para normalizar distribuci√≥n sesgada
- M√∫ltiples loss functions (RMSE, MAE, Quantile) para robustez a outliers
- Validaci√≥n cruzada estratificada por buckets de duraci√≥n
- Comparaci√≥n con baseline (LinearRegression)
- M√©tricas en escala original para interpretabilidad
- An√°lisis detallado de residuos y errores

‚ö° VENTAJAS DE ESTA VERSI√ìN:
- ‚úÖ Funciona con cualquier dominio (IT, rural, construcci√≥n, etc.)
- ‚úÖ No requiere mapeo de categor√≠as
- ‚úÖ Features universales (duraci√≥n estimada, experiencia, carga, performance)
- ‚úÖ Menos overfitting a categor√≠as espec√≠ficas
- ‚úÖ M√°s generalizable

‚ö†Ô∏è TRADE-OFF:
- R¬≤ esperado: ~0.70-0.75 (vs ~0.85 con categ√≥ricas)
- A√∫n suficiente para producci√≥n (duration_est tiene correlaci√≥n ~0.9 con target)

Modelo Principal: CatBoostRegressor
Baseline: LinearRegression
Target: duration_real (d√≠as)

Output:
- Modelos entrenados en artifacts/numeric_only/
- M√©tricas comparativas en artifacts/regression_numeric_comparison.json
- Gr√°ficos y reportes en reports/regression_numeric_analysis/
"""

import os
import json
import warnings
warnings.filterwarnings("ignore")
from pathlib import Path
from datetime import datetime

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sqlalchemy import create_engine, text
from sqlalchemy.engine import URL

from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    mean_absolute_percentage_error
)
from sklearn.linear_model import LinearRegression

from catboost import CatBoostRegressor
import joblib
from scipy import stats

# ============================================================================
# CONFIGURACI√ìN
# ============================================================================

# Conexi√≥n a base de datos
HOST = os.getenv("MYSQL_HOST", "localhost")
DB   = os.getenv("MYSQL_DB", "sb")
USER = os.getenv("MYSQL_USER", "root")
PASS = os.getenv("MYSQL_PASS", "")
PORT = int(os.getenv("MYSQL_PORT", "3306"))

# Directorios de salida
ARTIFACT_DIR = Path("ml/models/duration")
REPORT_DIR   = Path("reports/regression_numeric_analysis")
ARTIFACT_DIR.mkdir(exist_ok=True, parents=True)
REPORT_DIR.mkdir(exist_ok=True, parents=True)

# Semilla para reproducibilidad
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# Configuraci√≥n de validaci√≥n cruzada
CV_FOLDS = 5
N_BOOTSTRAP = 100

# ============================================================================
# UTILIDADES
# ============================================================================

def print_section(title, char="=", width=80):
    """Imprime un t√≠tulo formateado."""
    print(f"\n{char * width}")
    print(f"{title}")
    print(f"{char * width}")

def save_json(data, filepath):
    """Guarda datos en formato JSON."""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"   üíæ Guardado: {filepath}")

def calculate_regression_metrics(y_true, y_pred, name="", y_pred_samples=None):
    """Calcula m√©tricas completas de regresi√≥n con intervalos de confianza."""
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    
    # MAPE con manejo de divisiones por cero
    y_true_nonzero = y_true[y_true != 0]
    y_pred_nonzero = y_pred[y_true != 0]
    if len(y_true_nonzero) > 0:
        mape = np.mean(np.abs((y_true_nonzero - y_pred_nonzero) / y_true_nonzero)) * 100
    else:
        mape = np.nan
    
    # Predicciones dentro de rangos
    within_2_days = np.mean(np.abs(y_true - y_pred) <= 2) * 100
    within_5_days = np.mean(np.abs(y_true - y_pred) <= 5) * 100
    within_10_days = np.mean(np.abs(y_true - y_pred) <= 10) * 100
    within_20_days = np.mean(np.abs(y_true - y_pred) <= 20) * 100
    
    # Calcular intervalos de confianza mediante bootstrap
    mae_ci = None
    rmse_ci = None
    if y_pred_samples is not None:
        mae_bootstrap = []
        rmse_bootstrap = []
        for _ in range(N_BOOTSTRAP):
            indices = np.random.choice(len(y_true), size=len(y_true), replace=True)
            mae_bootstrap.append(mean_absolute_error(y_true[indices], y_pred[indices]))
            rmse_bootstrap.append(np.sqrt(mean_squared_error(y_true[indices], y_pred[indices])))
        
        mae_ci = (np.percentile(mae_bootstrap, 2.5), np.percentile(mae_bootstrap, 97.5))
        rmse_ci = (np.percentile(rmse_bootstrap, 2.5), np.percentile(rmse_bootstrap, 97.5))
    
    metrics = {
        'mae': float(mae),
        'mae_ci_lower': float(mae_ci[0]) if mae_ci else None,
        'mae_ci_upper': float(mae_ci[1]) if mae_ci else None,
        'rmse': float(rmse),
        'rmse_ci_lower': float(rmse_ci[0]) if rmse_ci else None,
        'rmse_ci_upper': float(rmse_ci[1]) if rmse_ci else None,
        'r2': float(r2),
        'mape': float(mape) if not np.isnan(mape) else None,
        'within_2_days_pct': float(within_2_days),
        'within_5_days_pct': float(within_5_days),
        'within_10_days_pct': float(within_10_days),
        'within_20_days_pct': float(within_20_days)
    }
    
    if name:
        print(f"\n   üìà M√©tricas {name}:")
        if mae_ci:
            print(f"      ‚Ä¢ MAE:  {mae:.3f} d√≠as [IC 95%: {mae_ci[0]:.3f} - {mae_ci[1]:.3f}]")
        else:
            print(f"      ‚Ä¢ MAE:  {mae:.3f} d√≠as")
        if rmse_ci:
            print(f"      ‚Ä¢ RMSE: {rmse:.3f} d√≠as [IC 95%: {rmse_ci[0]:.3f} - {rmse_ci[1]:.3f}]")
        else:
            print(f"      ‚Ä¢ RMSE: {rmse:.3f} d√≠as")
        print(f"      ‚Ä¢ R¬≤:   {r2:.4f}")
        if not np.isnan(mape):
            print(f"      ‚Ä¢ MAPE: {mape:.2f}%")
        print(f"      ‚Ä¢ Dentro de ¬±2 d√≠as:  {within_2_days:.1f}%")
        print(f"      ‚Ä¢ Dentro de ¬±5 d√≠as:  {within_5_days:.1f}%")
        print(f"      ‚Ä¢ Dentro de ¬±10 d√≠as: {within_10_days:.1f}%")
        print(f"      ‚Ä¢ Dentro de ¬±20 d√≠as: {within_20_days:.1f}%")
    
    return metrics

def plot_predictions_vs_actual(y_true, y_pred, title, filename):
    """Gr√°fico de predicciones vs valores reales."""
    plt.figure(figsize=(10, 8))
    
    plt.scatter(y_true, y_pred, alpha=0.5, s=20, edgecolors='k', linewidths=0.5)
    
    min_val = min(y_true.min(), y_pred.min())
    max_val = max(y_true.max(), y_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Predicci√≥n Perfecta')
    
    plt.fill_between([min_val, max_val], 
                     [min_val - 2, max_val - 2], 
                     [min_val + 2, max_val + 2],
                     alpha=0.2, color='green', label='¬±2 d√≠as')
    plt.fill_between([min_val, max_val], 
                     [min_val - 5, max_val - 5], 
                     [min_val + 5, max_val + 5],
                     alpha=0.1, color='blue', label='¬±5 d√≠as')
    
    plt.xlabel('Duraci√≥n Real (d√≠as)', fontsize=12)
    plt.ylabel('Duraci√≥n Predicha (d√≠as)', fontsize=12)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.legend(loc='upper left', fontsize=10)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   üìä Gr√°fico guardado: {filename}")

def plot_residuals(y_true, y_pred, title, filename):
    """Gr√°fico de an√°lisis de residuos."""
    residuals = y_pred - y_true
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle(title, fontsize=16, fontweight='bold')
    
    # 1. Residuos vs Predicciones
    ax = axes[0, 0]
    ax.scatter(y_pred, residuals, alpha=0.5, s=20, edgecolors='k', linewidths=0.5)
    ax.axhline(y=0, color='r', linestyle='--', lw=2)
    ax.axhline(y=2, color='orange', linestyle=':', lw=1, label='¬±2 d√≠as')
    ax.axhline(y=-2, color='orange', linestyle=':', lw=1)
    ax.set_xlabel('Predicci√≥n (d√≠as)')
    ax.set_ylabel('Residuos (d√≠as)')
    ax.set_title('Residuos vs Predicciones')
    ax.legend()
    ax.grid(alpha=0.3)
    
    # 2. Histograma de residuos
    ax = axes[0, 1]
    ax.hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='steelblue')
    ax.axvline(x=0, color='r', linestyle='--', lw=2, label='Media ideal')
    ax.axvline(x=residuals.mean(), color='orange', linestyle='--', lw=2, 
               label=f'Media real: {residuals.mean():.2f}')
    ax.set_xlabel('Residuos (d√≠as)')
    ax.set_ylabel('Frecuencia')
    ax.set_title('Distribuci√≥n de Residuos')
    ax.legend()
    ax.grid(alpha=0.3)
    
    # 3. Q-Q Plot
    ax = axes[1, 0]
    stats.probplot(residuals, dist="norm", plot=ax)
    ax.set_title('Q-Q Plot (Normalidad de Residuos)')
    ax.grid(alpha=0.3)
    
    # 4. Residuos vs Valores reales
    ax = axes[1, 1]
    ax.scatter(y_true, residuals, alpha=0.5, s=20, edgecolors='k', linewidths=0.5)
    ax.axhline(y=0, color='r', linestyle='--', lw=2)
    ax.set_xlabel('Duraci√≥n Real (d√≠as)')
    ax.set_ylabel('Residuos (d√≠as)')
    ax.set_title('Residuos vs Valores Reales')
    ax.grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   üìä Gr√°fico guardado: {filename}")

def plot_feature_importance_regression(model, feature_names, model_name, filename, top_n=20):
    """Visualiza feature importance para regresi√≥n."""
    importances = model.get_feature_importance()
    
    indices = np.argsort(importances)[::-1][:top_n]
    top_features = [feature_names[i] for i in indices]
    top_importances = importances[indices]
    
    plt.figure(figsize=(10, 8))
    plt.barh(range(len(top_features)), top_importances, color='steelblue')
    plt.yticks(range(len(top_features)), top_features)
    plt.xlabel('Importancia', fontsize=12)
    plt.title(f'Top {top_n} Features - {model_name}', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   üìä Gr√°fico guardado: {filename}")

# ============================================================================
# CARGA Y PREPARACI√ìN DE DATOS
# ============================================================================

print_section("üöÄ ENTRENAMIENTO CATBOOST REGRESSOR - SOLO FEATURES NUM√âRICAS")

print("\n[1/8] Conectando a MySQL y cargando datos...")

try:
    url = URL.create(
        "mysql+pymysql",
        username=USER,
        password=PASS,
        host=HOST,
        port=PORT,
        database=DB,
        query={"charset": "utf8mb4"}
    )
    engine = create_engine(url, pool_pre_ping=True, connect_args={"connect_timeout": 10})
    print(f"   ‚úÖ Conectado a MySQL en {HOST}:{PORT}/{DB}")
except Exception as e:
    raise RuntimeError(f"‚ùå Error creando conexi√≥n a MySQL: {type(e).__name__}: {e}")

try:
    query = text("SELECT * FROM v_training_dataset_clean")
    df = pd.read_sql(query, engine)
    print(f"   üìä Total filas cargadas: {len(df):,}")
except Exception as e:
    raise RuntimeError(f"‚ùå Error ejecutando consulta SQL: {type(e).__name__}: {e}")
finally:
    engine.dispose()

# ============================================================================
# PREPARACI√ìN DEL TARGET
# ============================================================================

print("\n[2/8] Preparando variable objetivo (duration_real)...")

target_col = "duration_real"
if target_col not in df.columns:
    raise RuntimeError(f"‚ùå No se encontr√≥ la columna objetivo '{target_col}'")

df = df[~df[target_col].isna()].copy()
df = df[df[target_col] > 0].copy()

# Convertir de minutos a d√≠as
print(f"\n   üîß Conversi√≥n de unidades:")
print(f"      Promedio antes: {df[target_col].mean():,.2f} minutos")

df[target_col] = df[target_col] / (60 * 24)
df['duration_est_imputed'] = df['duration_est_imputed'] / (60 * 24)

print(f"      Promedio despu√©s: {df[target_col].mean():.2f} d√≠as")
print(f"      ‚úÖ Conversi√≥n aplicada: minutos ‚Üí d√≠as")

n_total = len(df)
duration_stats = df[target_col].describe()

print(f"   üìä Filas con target v√°lido: {n_total:,}")
print(f"\n   üìä Estad√≠sticas de duration_real:")
print(f"      Media:    {duration_stats['mean']:.2f} d√≠as")
print(f"      Mediana:  {duration_stats['50%']:.2f} d√≠as")
print(f"      M√≠n:      {duration_stats['min']:.2f} d√≠as")
print(f"      M√°x:      {duration_stats['max']:.2f} d√≠as")
print(f"      Std:      {duration_stats['std']:.2f} d√≠as")

skewness = stats.skew(df[target_col])
print(f"      Asimetr√≠a: {skewness:.2f} {'(sesgada a la derecha)' if skewness > 1 else '(sim√©trica)'}")

# ============================================================================
# SELECCI√ìN DE FEATURES - SOLO NUM√âRICAS
# ============================================================================

print_section("üéØ SELECCI√ìN DE FEATURES - SOLO NUM√âRICAS (UNIVERSALES)", char="-")

print("\n[3/8] Seleccionando features num√©ricas sin dependencias de dominio...")

# Features que causan data leakage
LEAKAGE_FEATURES = {
    "duration_real",
    "person_avg_delay_ratio",
    "task_success_rate",
    "completed_on_time_alt",
}

# ‚≠ê FEATURES NUM√âRICAS UNIVERSALES (funcionan en cualquier dominio)
NUMERIC_FEATURES = {
    # Caracter√≠sticas de la tarea
    "duration_est_imputed",  # ‚≠ê‚≠ê‚≠ê MUY IMPORTANTE (correlaci√≥n ~0.9)
    
    # Caracter√≠sticas de la persona
    "experience_years_imputed",       # A√±os de experiencia
    "availability_hours_week_imputed", # Disponibilidad semanal
    "current_load_imputed",            # Carga actual de trabajo
    "performance_index_imputed",       # √çndice de rendimiento (0-1)
    "rework_rate_imputed",             # Tasa de retrabajos (0-1)
    
    # M√©tricas derivadas
    "load_ratio",  # Ratio de carga (current_load / availability)
}

# Convertir complexity_level a num√©rico si existe
if 'complexity_level' in df.columns:
    # Intentar convertir a num√©rico
    df['complexity_numeric'] = pd.to_numeric(df['complexity_level'], errors='coerce')
    
    # Si hay valores textuales (Baja/Media/Alta), mapearlos
    if df['complexity_numeric'].isna().any():
        complexity_map = {
            'Baja': 1, 'baja': 1, 'LOW': 1, 'Low': 1,
            'Media': 2, 'media': 2, 'MEDIUM': 2, 'Medium': 2,
            'Alta': 3, 'alta': 3, 'HIGH': 3, 'High': 3,
        }
        df['complexity_numeric'] = df['complexity_level'].map(complexity_map).fillna(df['complexity_numeric'])
    
    # Si a√∫n hay valores num√©ricos grandes (100, 200, 400), normalizarlos
    if df['complexity_numeric'].max() > 10:
        # Normalizar a escala 1-3
        min_val = df['complexity_numeric'].min()
        max_val = df['complexity_numeric'].max()
        df['complexity_numeric'] = 1 + 2 * (df['complexity_numeric'] - min_val) / (max_val - min_val)
    
    NUMERIC_FEATURES.add('complexity_numeric')
    print(f"   ‚úÖ complexity_level convertido a complexity_numeric (escala 1-3)")

# Filtrar features que existen en el DataFrame
feature_cols = [c for c in df.columns if c in NUMERIC_FEATURES]

print(f"\n   ‚úÖ Features num√©ricas seleccionadas: {len(feature_cols)}")
for col in sorted(feature_cols):
    print(f"      ‚Ä¢ {col}")

print(f"\n   ‚ùå Features categ√≥ricas ELIMINADAS (evitar dependencia de dominio):")
categorical_excluded = ['task_area', 'task_type', 'person_area', 'role']
for col in categorical_excluded:
    print(f"      ‚Ä¢ {col} (espec√≠fico del dominio)")

# Verificar correlaci√≥n con target
print(f"\n   üìä Correlaciones con duration_real:")
correlations = df[[target_col] + feature_cols].corr()[target_col].sort_values(ascending=False)
for col in feature_cols:
    corr = correlations[col]
    print(f"      ‚Ä¢ {col:<35} {corr:>6.3f}")

# Guardar configuraci√≥n
columns_config = {
    "numeric": feature_cols,
    "categorical": [],  # No se usan categ√≥ricas
    "excluded_categorical": categorical_excluded,
    "excluded_leakage": list(LEAKAGE_FEATURES),
    "target": target_col,
    "target_stats": {
        "mean": float(duration_stats['mean']),
        "median": float(duration_stats['50%']),
        "std": float(duration_stats['std']),
        "min": float(duration_stats['min']),
        "max": float(duration_stats['max']),
        "skewness": float(skewness)
    },
    "note": "Modelo entrenado SOLO con features num√©ricas para generalizaci√≥n cross-domain"
}
save_json(columns_config, ARTIFACT_DIR / "columns_regression_numeric.json")

# ============================================================================
# PREPARAR X E Y
# ============================================================================

print("\n[4/8] Preparando datos con transformaci√≥n logar√≠tmica...")

X = df[feature_cols].copy()
y = df[target_col].copy()

# Imputar valores faltantes en X
print(f"\n   üîß Imputaci√≥n de valores faltantes:")
for col in feature_cols:
    if X[col].isna().any() or X[col].isna().all():
        n_missing = X[col].isna().sum()
        
        # Si toda la columna es NaN, usar 0
        if X[col].isna().all():
            X[col] = 0.0
            print(f"      ‚Ä¢ {col}: {n_missing} NaN ‚Üí 0.0 (columna vac√≠a)")
        else:
            # Usar mediana si hay valores v√°lidos
            median_val = X[col].median()
            if pd.isna(median_val):
                # Si la mediana tambi√©n es NaN, usar 0
                X[col] = X[col].fillna(0.0)
                print(f"      ‚Ä¢ {col}: {n_missing} NaN ‚Üí 0.0 (mediana inv√°lida)")
            else:
                X[col] = X[col].fillna(median_val)
                print(f"      ‚Ä¢ {col}: {n_missing} NaN ‚Üí {median_val:.2f} (mediana)")

# Transformaci√≥n log del target
y_log = np.log1p(y)

print(f"\n   üìä Shape de X: {X.shape}")
print(f"   üìä Shape de y: {y.shape}")
print(f"   üìä Target transformado: log1p(duration_real)")
print(f"      Rango original: [{y.min():.2f}, {y.max():.2f}] d√≠as")
print(f"      Rango log:      [{y_log.min():.2f}, {y_log.max():.2f}]")

# Split estratificado
print("\n   üìä Creando split estratificado por duraci√≥n...")
duration_bins = pd.qcut(y, q=5, labels=False, duplicates='drop')

X_train, X_test, y_train_log, y_test_log, y_train, y_test = train_test_split(
    X, y_log, y,
    test_size=0.2, 
    random_state=RANDOM_STATE, 
    stratify=duration_bins
)

print(f"   üìä Train: {len(X_train):,} filas")
print(f"   üìä Test:  {len(X_test):,} filas")
print(f"   üìä Media duraci√≥n train: {y_train.mean():.2f} d√≠as")
print(f"   üìä Media duraci√≥n test:  {y_test.mean():.2f} d√≠as")

# ============================================================================
# MODELO 1: LINEAR REGRESSION (BASELINE)
# ============================================================================

print_section("üìä MODELO 1: LINEAR REGRESSION (BASELINE)", char="-")

print("\n[5/8] Entrenando LinearRegression como baseline...")

# Normalizar features para LinearRegression
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

lr_model = LinearRegression()
lr_model.fit(X_train_scaled, y_train_log)

lr_y_pred_log = lr_model.predict(X_test_scaled)
lr_y_pred = np.expm1(lr_y_pred_log)
y_test_original = y_test.values

lr_metrics = calculate_regression_metrics(y_test_original, lr_y_pred, "Linear Regression")

joblib.dump({'model': lr_model, 'scaler': scaler}, ARTIFACT_DIR / "model_linear_regression_numeric.pkl")
print(f"\n   üíæ Modelo guardado: {ARTIFACT_DIR / 'model_linear_regression_numeric.pkl'}")

plot_predictions_vs_actual(
    y_test_original, lr_y_pred,
    "Linear Regression (Num√©ricas Only) - Predicci√≥n vs Real",
    REPORT_DIR / "predictions_vs_actual_lr_numeric.png"
)

plot_residuals(
    y_test_original, lr_y_pred,
    "Linear Regression (Num√©ricas Only) - An√°lisis de Residuos",
    REPORT_DIR / "residuals_lr_numeric.png"
)

# ============================================================================
# MODELO 2: CATBOOST RMSE (PRINCIPAL)
# ============================================================================

print_section("üöÄ MODELO 2: CATBOOST RMSE (PRINCIPAL - NUM√âRICAS ONLY)", char="-")

print("\n[6/8] Entrenando CatBoost con loss='RMSE' (solo features num√©ricas)...")

catboost_rmse = CatBoostRegressor(
    loss_function='RMSE',
    eval_metric='RMSE',
    iterations=1500,
    learning_rate=0.03,
    depth=7,
    l2_leaf_reg=3,
    random_seed=RANDOM_STATE,
    verbose=100,
    early_stopping_rounds=100,
    bagging_temperature=0.5,
    random_strength=1.0
)

catboost_rmse.fit(
    X_train, y_train_log,
    eval_set=(X_test, y_test_log),
    verbose=100
)

print("\n   üîÑ Validaci√≥n cruzada con 5 folds...")
cv_scores = []
kfold = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)
for fold_num, (train_idx, val_idx) in enumerate(kfold.split(X_train), 1):
    print(f"   üìä Procesando fold {fold_num}/{CV_FOLDS}...", end=" ")
    
    X_fold_train = X_train.iloc[train_idx]
    X_fold_val = X_train.iloc[val_idx]
    y_fold_train = y_train_log.iloc[train_idx]
    y_fold_val = y_train_log.iloc[val_idx]
    
    model_cv = CatBoostRegressor(
        loss_function='RMSE',
        iterations=300,
        learning_rate=0.05,
        depth=7,
        l2_leaf_reg=3,
        random_seed=RANDOM_STATE,
        verbose=0
    )
    model_cv.fit(X_fold_train, y_fold_train, verbose=0)
    y_val_pred_log = model_cv.predict(X_fold_val)
    y_val_pred = np.expm1(y_val_pred_log)
    y_val_true = np.expm1(y_fold_val)
    
    mae_fold = mean_absolute_error(y_val_true, y_val_pred)
    cv_scores.append(mae_fold)
    print(f"MAE: {mae_fold:.2f} d√≠as ‚úÖ")

cv_mean = np.mean(cv_scores)
cv_std = np.std(cv_scores)
print(f"   ‚úÖ CV MAE promedio: {cv_mean:.3f} ¬± {cv_std:.3f} d√≠as")

# Predicciones
cb_rmse_y_pred_log = catboost_rmse.predict(X_test)
cb_rmse_y_pred = np.expm1(cb_rmse_y_pred_log)

cb_rmse_metrics = calculate_regression_metrics(y_test_original, cb_rmse_y_pred, "CatBoost RMSE (Numeric)", y_pred_samples=True)
cb_rmse_metrics['cv_mae_mean'] = float(cv_mean)
cb_rmse_metrics['cv_mae_std'] = float(cv_std)

# Mejora vs baseline
print(f"\n   üìä Mejora vs Linear Regression:")
for metric in ['mae', 'rmse', 'r2']:
    if metric == 'r2':
        improvement = (cb_rmse_metrics[metric] - lr_metrics[metric]) / abs(lr_metrics[metric]) * 100
    else:
        improvement = -(cb_rmse_metrics[metric] - lr_metrics[metric]) / lr_metrics[metric] * 100
    symbol = "üìà" if improvement > 0 else "üìâ"
    print(f"      {symbol} {metric.upper()}: {improvement:+.2f}%")

# Guardar modelo
joblib.dump(catboost_rmse, ARTIFACT_DIR / "model_catboost_rmse_numeric.pkl")
print(f"\n   üíæ Modelo guardado: {ARTIFACT_DIR / 'model_catboost_rmse_numeric.pkl'}")

plot_predictions_vs_actual(
    y_test_original, cb_rmse_y_pred,
    "CatBoost RMSE (Num√©ricas Only) - Predicci√≥n vs Real",
    REPORT_DIR / "predictions_vs_actual_catboost_numeric.png"
)

plot_residuals(
    y_test_original, cb_rmse_y_pred,
    "CatBoost RMSE (Num√©ricas Only) - An√°lisis de Residuos",
    REPORT_DIR / "residuals_catboost_numeric.png"
)

plot_feature_importance_regression(
    catboost_rmse, feature_cols, "CatBoost RMSE (Num√©ricas Only)",
    REPORT_DIR / "feature_importance_catboost_numeric.png", top_n=len(feature_cols)
)

# ============================================================================
# MODELO 3: CATBOOST MAE (ROBUSTO)
# ============================================================================

print_section("üìä MODELO 3: CATBOOST MAE (ROBUSTO A OUTLIERS)", char="-")

print("\n[7/8] Entrenando CatBoost con loss='MAE'...")

catboost_mae = CatBoostRegressor(
    loss_function='MAE',
    eval_metric='MAE',
    iterations=1000,
    learning_rate=0.05,
    depth=6,
    l2_leaf_reg=3,
    random_seed=RANDOM_STATE,
    verbose=False,
    early_stopping_rounds=50
)

catboost_mae.fit(
    X_train, y_train_log,
    eval_set=(X_test, y_test_log),
    verbose=False
)

cb_mae_y_pred_log = catboost_mae.predict(X_test)
cb_mae_y_pred = np.expm1(cb_mae_y_pred_log)

cb_mae_metrics = calculate_regression_metrics(y_test_original, cb_mae_y_pred, "CatBoost MAE (Numeric)")

joblib.dump(catboost_mae, ARTIFACT_DIR / "model_catboost_mae_numeric.pkl")
print(f"\n   üíæ Modelo guardado: {ARTIFACT_DIR / 'model_catboost_mae_numeric.pkl'}")

# ============================================================================
# COMPARACI√ìN FINAL
# ============================================================================

print_section("üìä COMPARACI√ìN FINAL - MODELOS NUM√âRICOS", char="-")

print("\n   üìä Tabla Comparativa de M√©tricas:\n")
print("   " + "-" * 100)
print(f"   {'Modelo':<30} {'MAE':>10} {'RMSE':>10} {'R¬≤':>8} {'¬±5d√≠as':>10} {'¬±10d√≠as':>10}")
print("   " + "-" * 100)

models_results = {
    'Linear Regression': lr_metrics,
    'CatBoost RMSE (Numeric)': cb_rmse_metrics,
    'CatBoost MAE (Numeric)': cb_mae_metrics,
}

for model_name, metrics in models_results.items():
    print(f"   {model_name:<30} {metrics['mae']:>10.3f} {metrics['rmse']:>10.3f} "
          f"{metrics['r2']:>8.4f} {metrics['within_5_days_pct']:>9.1f}% "
          f"{metrics['within_10_days_pct']:>9.1f}%")

print("   " + "-" * 100)

# Determinar mejor modelo
best_model = max(models_results.items(), key=lambda x: x[1]['r2'])
print(f"\n   üèÜ Mejor modelo (por R¬≤): {best_model[0]}")
print(f"   üéØ R¬≤ = {best_model[1]['r2']:.4f} (explica {best_model[1]['r2']*100:.1f}% de la varianza)")

# Guardar comparaci√≥n
comparison_data = {
    'timestamp': datetime.now().isoformat(),
    'model_type': 'numeric_only',
    'note': 'Entrenado solo con features num√©ricas para generalizaci√≥n cross-domain',
    'dataset_info': {
        'total_samples': n_total,
        'train_samples': len(X_train),
        'test_samples': len(X_test),
        'target_stats': columns_config['target_stats'],
        'features_used': feature_cols
    },
    'models': models_results,
    'best_model': {
        'name': best_model[0],
        'metrics': best_model[1]
    }
}

save_json(comparison_data, ARTIFACT_DIR / "regression_numeric_comparison.json")

# Gr√°fico comparativo
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

metrics_to_plot = ['mae', 'rmse', 'r2']
titles = ['MAE (d√≠as) - Menor es mejor', 'RMSE (d√≠as) - Menor es mejor', 'R¬≤ - Mayor es mejor']

for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):
    ax = axes[idx]
    model_names = list(models_results.keys())
    values = [models_results[name][metric] for name in model_names]
    
    colors = ['lightblue' if name != best_model[0] else 'green' for name in model_names]
    bars = ax.bar(range(len(model_names)), values, color=colors, edgecolor='black', alpha=0.7)
    
    ax.set_xticks(range(len(model_names)))
    ax.set_xticklabels(model_names, rotation=45, ha='right')
    ax.set_ylabel(metric.upper(), fontsize=11)
    ax.set_title(title, fontsize=12, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    
    for bar, value in zip(bars, values):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{value:.3f}', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.savefig(REPORT_DIR / "models_comparison_numeric.png", dpi=300, bbox_inches='tight')
plt.close()
print(f"\n   üìä Gr√°fico comparativo guardado: models_comparison_numeric.png")

# ============================================================================
# REPORTE FINAL
# ============================================================================

print_section("‚úÖ ENTRENAMIENTO COMPLETADO - MODELO NUM√âRICO", char="=")

print(f"""
üìÅ Archivos generados:

Modelos entrenados ({ARTIFACT_DIR}):
   ‚Ä¢ model_catboost_rmse_numeric.pkl ‚≠ê RECOMENDADO
   ‚Ä¢ model_catboost_mae_numeric.pkl
   ‚Ä¢ model_linear_regression_numeric.pkl

Configuraci√≥n y m√©tricas:
   ‚Ä¢ columns_regression_numeric.json
   ‚Ä¢ regression_numeric_comparison.json

Visualizaciones ({REPORT_DIR}):
   ‚Ä¢ predictions_vs_actual_catboost_numeric.png
   ‚Ä¢ residuals_catboost_numeric.png
   ‚Ä¢ feature_importance_catboost_numeric.png
   ‚Ä¢ models_comparison_numeric.png

üìä Resumen de Resultados:

Mejor modelo: {best_model[0]}
   ‚Ä¢ R¬≤ = {best_model[1]['r2']:.4f} (explica {best_model[1]['r2']*100:.1f}% de la varianza)
   ‚Ä¢ MAE = {best_model[1]['mae']:.2f} d√≠as (error promedio)
   ‚Ä¢ RMSE = {best_model[1]['rmse']:.2f} d√≠as
   ‚Ä¢ Dentro de ¬±5 d√≠as:  {best_model[1]['within_5_days_pct']:.1f}%
   ‚Ä¢ Dentro de ¬±10 d√≠as: {best_model[1]['within_10_days_pct']:.1f}%

üéØ Caracter√≠sticas del Modelo Num√©rico:
   ‚úÖ Compatible con CUALQUIER dominio (IT, rural, construcci√≥n, etc.)
   ‚úÖ No requiere mapeo de categor√≠as espec√≠ficas de dominio
   ‚úÖ Features universales: duraci√≥n estimada, experiencia, carga, performance
   ‚úÖ Menor overfitting, mejor generalizaci√≥n
   ‚úÖ Validaci√≥n cruzada: {cb_rmse_metrics.get('cv_mae_mean', 0):.2f} ¬± {cb_rmse_metrics.get('cv_mae_std', 0):.2f} d√≠as

‚ö†Ô∏è Trade-off vs Modelo con Categ√≥ricas:
   ‚Ä¢ R¬≤ esperado: ~{best_model[1]['r2']:.2f} (vs ~0.85 con categ√≥ricas)
   ‚Ä¢ A√∫n SUFICIENTE para producci√≥n
   ‚Ä¢ duration_est tiene correlaci√≥n {correlations.get('duration_est_imputed', 0):.3f} con target

üîó Uso en producci√≥n:

   import joblib
   import numpy as np
   import pandas as pd
   
   # Cargar modelo
   model = joblib.load('artifacts/numeric_only/model_catboost_rmse_numeric.pkl')
   
   # Preparar datos (SOLO num√©ricas)
   X_new = pd.DataFrame([{{
       'duration_est_imputed': 10.0,  # d√≠as
       'complexity_numeric': 2.0,      # 1=Baja, 2=Media, 3=Alta
       'experience_years_imputed': 5.0,
       'availability_hours_week_imputed': 40.0,
       'current_load_imputed': 80.0,
       'performance_index_imputed': 0.85,
       'rework_rate_imputed': 0.15,
       'load_ratio': 2.0
   }}])
   
   # Predecir (deshacer transformaci√≥n log)
   y_pred_log = model.predict(X_new)
   y_pred_dias = np.expm1(y_pred_log)[0]
   
   print(f"Duraci√≥n estimada: {{y_pred_dias:.1f}} d√≠as")

üí° Ventajas de esta versi√≥n:
   ‚Ä¢ Funciona desde d√≠a 1 con TUS datos de producci√≥n
   ‚Ä¢ No necesitas re-mapear task_area, task_type, person_area, role
   ‚Ä¢ Compatible con dominio IT/Software (tu caso de uso)
   ‚Ä¢ Puedes ir mejorando con datos reales despu√©s

üéì Para tu tesis:
   ‚Ä¢ Justifica el enfoque "domain-agnostic" para generalizaci√≥n
   ‚Ä¢ Documenta el trade-off precisi√≥n vs generalizaci√≥n
   ‚Ä¢ Compara con modelo con categ√≥ricas (datos gubernamentales)
   ‚Ä¢ Explica por qu√© features num√©ricas son m√°s transferibles
""")

print("\n" + "=" * 80)
print("üéâ ¬°Modelo num√©rico entrenado exitosamente!")
print("=" * 80 + "\n")
